{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e077bf73-cae2-46fa-b2e8-420ea1c341f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "from utils import get_shd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9940d2-13e8-4bd1-93eb-ecadf57a1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coarse network structure and the time steps are dicated by the SHD dataset. \n",
    "nb_inputs  = 700\n",
    "nb_hidden  = 200\n",
    "nb_outputs = 20\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps = 100\n",
    "max_time = 1.4\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e37958-65aa-40a9-b1b6-9e82f7ca000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b86715-0f52-42ca-97f1-e3471f2dd08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available at: /home/jason/data/hdspikes/shd_train.h5\n",
      "Available at: /home/jason/data/hdspikes/shd_test.h5\n"
     ]
    }
   ],
   "source": [
    "# Here we load the Dataset\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "get_shd_dataset(cache_dir, cache_subdir)\n",
    "\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "x_train = train_file['spikes']\n",
    "y_train = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aaccf9d-bcd5-44e6-b39f-abca132c6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the plots and accuracies if it doesn't exist\n",
    "output_dir = './training_outputs'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ed58ea-501d-4eba-807b-95f3c63c366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors. \n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y, dtype=int)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "    \n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            \n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "    \n",
    "        X_batch = torch.sparse_coo_tensor(i, v, torch.Size([batch_size, nb_steps, nb_units]), dtype=torch.float, device=device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb90bbd-ebce-4142-af9c-f7cd57bf0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d8c025-800c-4a96-be97-bf2beb19cf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init done\n"
     ]
    }
   ],
   "source": [
    "weight_scale = 0.2\n",
    "\n",
    "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f75e65a-abb9-4e8b-aa97-10102c00ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
    "    gs=GridSpec(*dim)\n",
    "    if spk is not None:\n",
    "        dat = 1.0*mem\n",
    "        dat[spk>0.0] = spike_height\n",
    "        dat = dat.detach().cpu().numpy()\n",
    "    else:\n",
    "        dat = mem.detach().cpu().numpy()\n",
    "    for i in range(np.prod(dim)):\n",
    "        if i==0: a0=ax=plt.subplot(gs[i])\n",
    "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
    "        ax.plot(dat[i])\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ab5b8e-cf71-4f87-8e9d-34cdfbe8786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def live_plot(loss):\n",
    "    if len(loss) == 1:\n",
    "        return\n",
    "    clear_output(wait=True)\n",
    "    ax = plt.figure(figsize=(3,2), dpi=150).gca()\n",
    "    ax.plot(range(1, len(loss) + 1), loss)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e240f1-47c5-47a0-86d9-bb892124a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(loss_hist, config_index, output_dir='./training_outputs'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_hist, label='Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss History for Configuration {config_index}')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_plot_config_{config_index}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba865e4-f18f-4dd2-9569-4b0684140f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to store the test accuracies\n",
    "accuracy_file_path = os.path.join(output_dir, 'test_accuracies.txt')\n",
    "\n",
    "# Function to append accuracies to a file\n",
    "def log_accuracy(config_index, accuracy, accuracy_file_path):\n",
    "    with open(accuracy_file_path, 'a') as file:\n",
    "        file.write(f\"Configuration {config_index}: Test Accuracy = {accuracy:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9dbfa6-ed3c-422c-8475-61cd757601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "    \n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"labels_\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "    \n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80c0f36d-d873-48fa-95d9-44e03a6b1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
    "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    for t in range(nb_steps):\n",
    "        h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
    "        mthr = mem-1.0\n",
    "        out = spike_fn(mthr)\n",
    "        rst = out.detach() # We do not want to backprop through the reset\n",
    "\n",
    "        new_syn = alpha*syn +h1\n",
    "        new_mem =(beta*mem +syn)*(1.0-rst)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "        \n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec,dim=1)\n",
    "    spk_rec = torch.stack(spk_rec,dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):\n",
    "        new_flt = alpha*flt +h2[:,t]\n",
    "        new_out = beta*out +flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec,dim=1)\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "    return out_rec, other_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eeed8ba-6277-463e-a98a-19010f17d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_data, y_data, lr, batch_size, nb_hidden, reg_strength, nb_epochs=10,config_index=0):\n",
    "    params = [w1, w2, v1]\n",
    "    optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time):\n",
    "            output, recs = run_snn(x_local.to_dense())\n",
    "            _, spks = recs\n",
    "            m, _ = torch.max(output, 1)\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "\n",
    "            # Regularizer loss\n",
    "            reg_loss = reg_strength * torch.sum(spks)  # L1 loss on total number of spikes\n",
    "            reg_loss += reg_strength * torch.mean(torch.sum(torch.sum(spks, dim=0), dim=0) ** 2)  # L2 loss on spikes per neuron\n",
    "            \n",
    "            # Combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        live_plot(loss_hist)\n",
    "        if(config_count != 0):\n",
    "            print(\"Epoch %i: loss=%.5f\" % (e + 1, mean_loss),f\"Testing configuration: {config_index}\")\n",
    "        else:\n",
    "            print(\"Epoch %i: loss=%.5f\" % (e + 1, mean_loss))\n",
    "        \n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7741a9f-3cc6-4d3d-b4d5-d1ca4f859393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWeights():\n",
    "    # Reinitialize weights here to ensure they're reset for every new run\n",
    "    w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "    \n",
    "    w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "    v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
    "    torch.nn.init.normal_(v1, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f29f7bc4-c453-4e3a-b8f1-98960661850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(x_data, y_data):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=False):\n",
    "        output,_ = run_snn(x_local.to_dense())\n",
    "        m,_= torch.max(output,1) # max over time\n",
    "        _,am=torch.max(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ed4a65-2119-411d-a028-a8a0765a51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations generated: 2\n",
      "{'lr': 1e-05, 'nb_hidden': 100, 'reg_strength': 0}\n",
      "{'lr': 0.0001, 'nb_hidden': 100, 'reg_strength': 0}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# param_space = {\n",
    "#     'lr': [1e-5, 1e-4, 1e-3, 5e-3, 1e-2],\n",
    "#     'nb_hidden': [100,200,300,400,500],\n",
    "#     'reg_strength': [0, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "# }\n",
    "\n",
    "param_space = {\n",
    "    'lr': [1e-5, 1e-4],\n",
    "    'nb_hidden': [100],\n",
    "    'reg_strength': [0],\n",
    "}\n",
    "\n",
    "# Using itertools.product to generate all possible combinations\n",
    "all_combinations = list(itertools.product(\n",
    "    param_space['lr'], \n",
    "    param_space['nb_hidden'], \n",
    "    param_space['reg_strength']\n",
    "))\n",
    "\n",
    "# Converting tuples from product to dictionaries\n",
    "configs = [\n",
    "    {'lr': lr, 'nb_hidden': nb_hidden, 'reg_strength': reg_strength} \n",
    "    for lr, nb_hidden, reg_strength in all_combinations\n",
    "]\n",
    "\n",
    "# Verify the length of configs and print the first few to check\n",
    "print(\"Total configurations generated:\", len(configs))\n",
    "for config in configs[:5]:\n",
    "    print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da789c46-0bb1-44c9-91a5-994b6582f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration 1 of 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(configs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting configuration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(configs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     loss_hist \u001b[38;5;241m=\u001b[39m train(x_train, y_train, lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, nb_hidden\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_hidden\u001b[39m\u001b[38;5;124m'\u001b[39m], reg_strength\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_strength\u001b[39m\u001b[38;5;124m'\u001b[39m], config_index\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, nb_epochs\u001b[38;5;241m=\u001b[39mnb_epochs)\n\u001b[1;32m      8\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m compute_classification_accuracy(x_test, y_test)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy for configuration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x_data, y_data, lr, batch_size, nb_hidden, reg_strength, nb_epochs, config_index)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(mean_loss)\n\u001b[1;32m     30\u001b[0m live_plot(loss_hist)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(config_count \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m: loss=\u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, mean_loss),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting configuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config_count' is not defined"
     ]
    }
   ],
   "source": [
    "# Test different hyperparameter configurations to 20 epochs each\n",
    "nb_epochs = 2\n",
    "best_test_accuracy = 0\n",
    "best_config = configs[0]\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Testing configuration {i+1} of {len(configs)}\")\n",
    "    loss_hist = train(x_train, y_train, lr=config['lr'], batch_size=256, nb_hidden=config['nb_hidden'], reg_strength=config['reg_strength'], config_index=i+1, nb_epochs=nb_epochs)\n",
    "    test_accuracy = compute_classification_accuracy(x_test, y_test)\n",
    "    print(f\"Test accuracy for configuration {i+1}: {test_accuracy:.3f}\")\n",
    "    log_accuracy(i+1, test_accuracy, accuracy_file_path)  # Log accuracy to file\n",
    "    save_plot(loss_hist, i+1)  # Save the loss plot\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        print(\"New best!\")\n",
    "        best_test_accuracy = test_accuracy\n",
    "        best_config = config\n",
    "initWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc87bb-f101-4041-946c-27e2b8c90918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model on 500 epochs\n",
    "nb_epochs = 5\n",
    "start_time = time.time()\n",
    "loss_hist = train(x_train, y_train, lr=best_config['lr'], batch_size=256, nb_hidden=best_config['nb_hidden'], reg_strength=best_config['reg_strength'], nb_epochs=nb_epochs)\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd61c4-0c69-4b27-9f32-81c0200fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed in {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2cf6cc-5fc9-4e54-a631-d9545fa682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train)))\n",
    "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test)))\n",
    "print(f\"Best hyperparameter config:{best_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4333efc-046a-4f0f-8dbd-8aa40e35c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch(x_data, y_data, shuffle=False):\n",
    "    for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=shuffle):\n",
    "        return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decd78d-da90-4fc9-a4b7-a6cc55d0a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
    "output, other_recordings = run_snn(x_batch.to_dense())\n",
    "mem_rec, spk_rec = other_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91632d1-0b9e-4c19-901c-59ca8dc1c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(dpi=100)\n",
    "plot_voltage_traces(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013985a8-6344-47d0-9909-2c58614bd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the hiddden layer spiking activity for some input stimuli\n",
    "\n",
    "nb_plt = 4\n",
    "gs = GridSpec(1,nb_plt)\n",
    "fig= plt.figure(figsize=(7,3),dpi=150)\n",
    "for i in range(nb_plt):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.imshow(spk_rec[i].detach().cpu().numpy().T,cmap=plt.cm.gray_r, origin=\"lower\" )\n",
    "    if i==0:\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Units\")\n",
    "\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6ec64-51a6-4ec1-b2c3-a6a3543c0eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
